{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abduloo7454/XAI-Hyper-GCN/blob/main/Hyper_CSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper-CSP"
      ],
      "metadata": {
        "id": "dzgdiwvagvm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "data = loadmat('/content/drive/MyDrive/GRAPH EEG/hyperCSP.mat')\n",
        "data = data['FeaturesAndLabels']\n",
        "\n",
        "## Experiment 1,2,3,4,5\n",
        "df = data[:,:30]\n",
        "label = data[:,-1]\n",
        "\n",
        "## Experiment 1,2\n",
        "df = df[:464,:]\n",
        "label = label[:464]"
      ],
      "metadata": {
        "id": "wtDBskoogd_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape, label.shape"
      ],
      "metadata": {
        "id": "CTyWVLbBhZdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Ensure df is a DataFrame and label is a Series\n",
        "df = pd.DataFrame(df)\n",
        "label = pd.Series(label)\n",
        "\n",
        "# ðŸš€ Feature Scaling (Standardization)\n",
        "# scaler = StandardScaler()\n",
        "# df_scaled = scaler.fit_transform(df)  # Scale features\n",
        "\n",
        "# ðŸš€ Split dataset into Train (75%), Validation (15%), and Test (10%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df, label, test_size=0.25, stratify=label, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.4, stratify=y_temp, random_state=42)\n",
        "\n",
        "# K-Fold Cross Validation (k=5) on the training set\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# ðŸš€ **Hyperparameter Tuning**\n",
        "svm_params = {'C': [0.1, 5, 10], 'kernel': ['linear', 'rbf', 'poly']}\n",
        "knn_params = {'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 10, 12]}\n",
        "lda_params = {'solver': ['lsqr', 'eigen'], 'shrinkage': ['auto', None]}  # âœ… FIX: Use correct solvers\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"SVM\": GridSearchCV(SVC(), svm_params, cv=5, scoring='accuracy'),\n",
        "    \"LDA\": GridSearchCV(LinearDiscriminantAnalysis(), lda_params, cv=5, scoring='accuracy'),\n",
        "    \"KNN\": GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, scoring='accuracy')\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    fold_accuracies = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train):\n",
        "        # Use .iloc to access rows by numerical index:\n",
        "        X_k_train, X_k_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_k_train, y_k_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_k_train, y_k_train)\n",
        "        best_model = model.best_estimator_\n",
        "        y_pred = best_model.predict(X_k_val)\n",
        "\n",
        "        acc = accuracy_score(y_k_val, y_pred)\n",
        "        fold_accuracies.append(acc)\n",
        "\n",
        "    # Train final model on the entire training set\n",
        "    best_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_val_pred = best_model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    val_precision = precision_score(y_val, y_val_pred, average='macro')\n",
        "    val_recall = recall_score(y_val, y_val_pred, average='macro')\n",
        "    val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
        "    test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
        "    test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    results[model_name] = {\n",
        "        \"Best Params\": model.best_params_,\n",
        "        \"Cross-Validation Accuracy\": np.mean(fold_accuracies),\n",
        "        \"Validation Accuracy\": val_accuracy,\n",
        "        \"Validation Precision\": val_precision,\n",
        "        \"Validation Recall\": val_recall,\n",
        "        \"Validation F1\": val_f1,\n",
        "        \"Test Accuracy\": test_accuracy,\n",
        "        \"Test Precision\": test_precision,\n",
        "        \"Test Recall\": test_recall,\n",
        "        \"Test F1\": test_f1\n",
        "    }\n",
        "\n",
        "# âœ… Print results properly (Fixing the TypeError)\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"\\nðŸ”¹ {model_name} Performance:\")\n",
        "    for metric, value in metrics.items():\n",
        "        if isinstance(value, dict):\n",
        "            print(f\"  {metric}: {value}\")  # Print dictionary properly\n",
        "        else:\n",
        "            print(f\"  {metric}: {value:.4f}\")  # Format numbers properly"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "8tFBYQP7bpOF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "071JKe-bCV5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}